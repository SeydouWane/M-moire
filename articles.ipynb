{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import newspaper\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors du traitement de l'article de https://kawtef.com/deces-de-la-maman-de-mbaye-dieye-faye-youssou-ndour-console-son-meilleur-ami_116030.html: Article `download()` failed with HTTPSConnectionPool(host='kawtef.com', port=443): Read timed out. on URL https://kawtef.com/deces-de-la-maman-de-mbaye-dieye-faye-youssou-ndour-console-son-meilleur-ami_116030.html\n",
      "Erreur lors du traitement de l'article de https://kawtef.com/serie-decheances-saison-2-episode-37-bande-annonce_116023.html: Article `download()` failed with HTTPSConnectionPool(host='kawtef.com', port=443): Read timed out. on URL https://kawtef.com/serie-decheances-saison-2-episode-37-bande-annonce_116023.html\n",
      "Erreur lors du traitement de l'article de https://www.dakaractu.com/New-Delhi-Le-president-Diomaye-exile-le-General-Souleymane-Kande-chef-d-Etat-major-de-l-armee-de-terre-comme-attache_a249076.html: HTTPSConnectionPool(host='www.dakaractu.com', port=443): Max retries exceeded with url: /New-Delhi-Le-president-Diomaye-exile-le-General-Souleymane-Kande-chef-d-Etat-major-de-l-armee-de-terre-comme-attache_a249076.html (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000022E2DFC82E0>: Failed to resolve 'www.dakaractu.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Erreur lors du traitement de l'article de https://www.dakaractu.com/Le-kg-du-riz-parfume-indien-connait-une-hausse-de-plus-de-50-Francs-CFA_a249074.html: Article `download()` failed with HTTPSConnectionPool(host='www.dakaractu.com', port=443): Max retries exceeded with url: /Le-kg-du-riz-parfume-indien-connait-une-hausse-de-plus-de-50-Francs-CFA_a249074.html (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000022E2DFCF1C0>: Failed to resolve 'www.dakaractu.com' ([Errno 11001] getaddrinfo failed)\")) on URL https://www.dakaractu.com/Le-kg-du-riz-parfume-indien-connait-une-hausse-de-plus-de-50-Francs-CFA_a249074.html\n",
      "Erreur lors du traitement de l'article de https://www.dakaractu.com/Secteur-extractif-les-cimentiers-meilleurs-eleves_a249115.html: ('Connection aborted.', OSError(0, 'Error'))\n",
      "Erreur lors du traitement de l'article de https://www.dakaractu.com/Lettre-a-Ousmane-Sonko-votre-j-accuse-contre-l-Europe-et-la-France-est-une-tartufferie-par-Moustapha-Diakhate_a248761.html: Article `download()` failed with HTTPSConnectionPool(host='www.dakaractu.com', port=443): Read timed out. (read timeout=7) on URL https://www.dakaractu.com/Lettre-a-Ousmane-Sonko-votre-j-accuse-contre-l-Europe-et-la-France-est-une-tartufferie-par-Moustapha-Diakhate_a248761.html\n",
      "Erreur lors du traitement de l'article de https://www.dakaractu.com/UAM-de-Diamniadio-La-visite-des-chantiers-que-nous-avons-faite-montre-effectivement-qu-avec-ce-soutien-a-moyen-terme_a248084.html: ('Connection aborted.', OSError(0, 'Error'))\n",
      "Erreur lors du traitement de l'article de https://www.dakaractu.com/Retour-apres-la-finale-de-la-CAN-2019-Les-Lions-attendus-a-Dakar-aux-alentours-de-14h_a173896.html: Article `download()` failed with HTTPSConnectionPool(host='www.dakaractu.com', port=443): Read timed out. (read timeout=7) on URL https://www.dakaractu.com/Retour-apres-la-finale-de-la-CAN-2019-Les-Lions-attendus-a-Dakar-aux-alentours-de-14h_a173896.html\n",
      "Erreur lors du traitement de l'article de https://www.dakaractu.com/C-etait-triste-et-tres-cruel-avec-beaucoup-de-deception-Edouard-Mendy-apres-la-finale_a173916.html: Article `download()` failed with HTTPSConnectionPool(host='www.dakaractu.com', port=443): Read timed out. (read timeout=7) on URL https://www.dakaractu.com/C-etait-triste-et-tres-cruel-avec-beaucoup-de-deception-Edouard-Mendy-apres-la-finale_a173916.html\n",
      "Erreur lors du traitement de l'article de https://www.dakaractu.com/Senegal-Tanzanie-2-0-Le-match-en-IMAGES_a172411.html: Article `download()` failed with HTTPSConnectionPool(host='www.dakaractu.com', port=443): Read timed out. (read timeout=7) on URL https://www.dakaractu.com/Senegal-Tanzanie-2-0-Le-match-en-IMAGES_a172411.html\n",
      "Erreur lors du traitement de l'article de https://www.dakaractu.com/Le-kg-du-riz-parfume-indien-connait-une-hausse-de-plus-de-50-Francs-CFA_a249074.html?com#comments: Article `download()` failed with HTTPSConnectionPool(host='www.dakaractu.com', port=443): Read timed out. (read timeout=7) on URL https://www.dakaractu.com/Le-kg-du-riz-parfume-indien-connait-une-hausse-de-plus-de-50-Francs-CFA_a249074.html?com#comments\n",
      "Erreur lors du traitement de l'article de https://www.pressafrik.com/La-justice-est-moins-boitillante-que-les-acteurs-politiques--Me-Oumar-Youm_a273608.html: Article `download()` failed with HTTPSConnectionPool(host='www.pressafrik.com', port=443): Read timed out. (read timeout=7) on URL https://www.pressafrik.com/La-justice-est-moins-boitillante-que-les-acteurs-politiques--Me-Oumar-Youm_a273608.html\n",
      "Erreur lors du traitement de l'article de https://www.pressafrik.com/Canicule-au-Cameroun-Il-y-a-eu-une-surmortalite-catastrophique-dans-le-Nord-affirme-une-figure-de-la-societe-civile_a273560.html: Article `download()` failed with HTTPSConnectionPool(host='www.pressafrik.com', port=443): Max retries exceeded with url: /Canicule-au-Cameroun-Il-y-a-eu-une-surmortalite-catastrophique-dans-le-Nord-affirme-une-figure-de-la-societe-civile_a273560.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000022E316811C0>, 'Connection to www.pressafrik.com timed out. (connect timeout=7)')) on URL https://www.pressafrik.com/Canicule-au-Cameroun-Il-y-a-eu-une-surmortalite-catastrophique-dans-le-Nord-affirme-une-figure-de-la-societe-civile_a273560.html\n",
      "Les données ont été enregistrées dans le fichier 'articles.json'.\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle de traitement du langage naturel français de SpaCy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def extract_triplets(article_text):\n",
    "    # Initialiser une liste pour stocker les triplets\n",
    "    triplets = []\n",
    "\n",
    "    # Analyser le texte de l'article avec SpaCy\n",
    "    doc = nlp(article_text)\n",
    "\n",
    "    # Parcourir les phrases du document\n",
    "    for sentence in doc.sents:\n",
    "        # Analyser la syntaxe de la phrase\n",
    "        parsed_sentence = nlp(sentence.text)\n",
    "\n",
    "        # Extraire les relations sujet-verbe-objet\n",
    "        for token in parsed_sentence:\n",
    "            if token.dep_ == \"nsubj\" and token.head.pos_ == \"VERB\":\n",
    "                sujet = token.text\n",
    "                predicat = token.head.lemma_\n",
    "                objet = None\n",
    "                for child in token.head.children:\n",
    "                    if child.dep_ == \"obj\":\n",
    "                        objet = child.text\n",
    "                        break\n",
    "\n",
    "                # Ajouter le triplet à la liste\n",
    "                triplet = f\"{sujet} {predicat} {objet}\"\n",
    "                triplets.append(triplet)\n",
    "\n",
    "    return triplets\n",
    "\n",
    "def extract_date_from_content(content):\n",
    "    # Fonction pour extraire une date à partir du contenu de l'article\n",
    "    possible_dates = []\n",
    "    for line in content.split('\\n'):\n",
    "        try:\n",
    "            date = datetime.strptime(line.strip(), \"%Y-%m-%d\")\n",
    "            possible_dates.append(date)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return possible_dates[0] if possible_dates else None\n",
    "\n",
    "def extract_date_from_meta(soup):\n",
    "    # Fonction pour extraire une date à partir des balises meta de l'article\n",
    "    date_meta_tags = ['article:published_time', 'date', 'DC.date.issued']\n",
    "    for tag in date_meta_tags:\n",
    "        meta = soup.find('meta', attrs={'property': tag}) or soup.find('meta', attrs={'name': tag})\n",
    "        if meta and meta.get('content'):\n",
    "            try:\n",
    "                return datetime.fromisoformat(meta['content'])\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def scrape_articles_from_links(file_path):\n",
    "    # Lire le fichier Excel contenant les liens des articles\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    article_data = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        site_url = row['site_url']\n",
    "        article_url = row['article_url']\n",
    "\n",
    "        try:\n",
    "            # Initialiser l'article avec Newspaper\n",
    "            article = newspaper.Article(article_url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            # Extraire les informations de l'article\n",
    "            titre = article.title\n",
    "            contenu = article.text\n",
    "            date_publication = article.publish_date\n",
    "            auteurs = article.authors\n",
    "\n",
    "            if date_publication is None:\n",
    "                # Tenter de récupérer la date à partir du contenu de l'article\n",
    "                date_publication = extract_date_from_content(contenu)\n",
    "\n",
    "                # Tenter de récupérer la date à partir des métadonnées\n",
    "                if date_publication is None:\n",
    "                    html = requests.get(article_url).text\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    date_publication = extract_date_from_meta(soup)\n",
    "\n",
    "            # Extraire les triplets du contenu de l'article\n",
    "            triplets = extract_triplets(contenu)\n",
    "\n",
    "            # Ajouter les données de l'article à la liste\n",
    "            article_data.append({\n",
    "                \"site_url\": site_url,\n",
    "                \"titre\": titre,\n",
    "                \"contenu\": contenu,\n",
    "                \"date\": date_publication.isoformat() if date_publication else None,\n",
    "                \"auteurs\": \", \".join(auteurs),\n",
    "                \"triplets\": triplets\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement de l'article de {article_url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Enregistrer les données dans un fichier JSON\n",
    "    with open('articles.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(article_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Les données ont été enregistrées dans le fichier 'articles.json'.\")\n",
    "\n",
    "# Chemin vers le fichier Excel contenant les liens des articles\n",
    "file_path = \"C:\\\\Users\\\\WANE\\\\Desktop\\\\Mémoire\\\\article_links.xlsx\"\n",
    "\n",
    "# Lancer le processus de scraping\n",
    "scrape_articles_from_links(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
